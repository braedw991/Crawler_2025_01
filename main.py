from crawler.vnexpress_crawler import crawl_articles_by_category_and_date
from exporter.pdf_exporter import export_pdf
# S·ª¨A ƒê·ªîI: Import t·ª´ th∆∞ m·ª•c 'integrations'
from integrations.google_drive_uploader import upload_to_drive
from database.db_manager import save_articles, load_articles
from config.settings import CATEGORIES, BASE_URL
import os
from datetime import datetime
import sys
import traceback
import json
from dotenv import load_dotenv
import pytz

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc c·ªßa d·ª± √°n v√†o sys.path ƒë·ªÉ import ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh
# ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng ƒë·ªÉ tr√°nh c√°c l·ªói ImportError trong c√°c m√¥i tr∆∞·ªùng kh√°c nhau
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# T·∫£i c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env (d√†nh cho ch·∫°y local)
load_dotenv()

# L·∫•y ID th∆∞ m·ª•c Drive t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
DRIVE_FOLDER_ID = os.getenv("DRIVE_FOLDER_ID")

def select_category():
    """Hi·ªÉn th·ªã danh s√°ch chuy√™n m·ª•c v√† cho ph√©p ng∆∞·ªùi d√πng ch·ªçn."""
    print("\n--- Vui l√≤ng ch·ªçn chuy√™n m·ª•c ƒë·ªÉ crawl ---")
    category_list = list(CATEGORIES.keys())
    for i, name in enumerate(category_list):
        print(f"{i + 1}. {name}")
    
    while True:
        try:
            choice_str = input(f"\n‚û°Ô∏è  Nh·∫≠p l·ª±a ch·ªçn c·ªßa b·∫°n (1-{len(category_list)}): ")
            if not choice_str: continue
            choice = int(choice_str)
            if 1 <= choice <= len(category_list):
                return category_list[choice - 1]
            else:
                print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá, vui l√≤ng nh·∫≠p l·∫°i.")
        except ValueError:
            print("‚ùå Vui l√≤ng nh·∫≠p m·ªôt con s·ªë.")

def get_target_date():
    """Cho ph√©p ng∆∞·ªùi d√πng nh·∫≠p ng√†y ho·∫∑c d√πng ng√†y h√¥m nay."""
    while True:
        date_str = input("üóìÔ∏è  Nh·∫≠p ng√†y mu·ªën crawl (DD-MM-YYYY), ho·∫∑c ƒë·ªÉ tr·ªëng ƒë·ªÉ l·∫•y ng√†y h√¥m nay: ").strip()
        if not date_str:
            target_date = datetime.now()
            print(f"‚úÖ S·ª≠ d·ª•ng ng√†y h√¥m nay: {target_date.strftime('%d-%m-%Y')}")
            return target_date
        else:
            try:
                return datetime.strptime(date_str, "%d-%m-%Y")
            except ValueError:
                print("‚ùå ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá. Vui l√≤ng d√πng DD-MM-YYYY.")

def main():
    """Quy tr√¨nh t·ª± ƒë·ªông h√≥a ch√≠nh."""
    # 1. D·ªçn d·∫πp d·ªØ li·ªáu c≈© ƒë·ªÉ b·∫Øt ƒë·∫ßu phi√™n l√†m vi·ªác m·ªõi
    save_articles([]) # T·∫°o ra file articles.json r·ªóng
    print("üßπ ƒê√£ d·ªçn d·∫πp d·ªØ li·ªáu c≈©, s·∫µn s√†ng cho phi√™n l√†m vi·ªác m·ªõi.")

    # 2. L·∫•y th√¥ng tin t·ª´ ng∆∞·ªùi d√πng
    category_name = select_category()
    target_date = get_target_date()

    # 3. Ch·∫°y Crawler
    crawl_articles_by_category_and_date(category_name, target_date)

    # Ki·ªÉm tra xem c√≥ b√†i vi·∫øt n√†o ƒë∆∞·ª£c crawl kh√¥ng
    if not load_articles():
        print("\n‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o cho chuy√™n m·ª•c v√† ng√†y ƒë√£ ch·ªçn. K·∫øt th√∫c ch∆∞∆°ng tr√¨nh.")
        return

    # 4. Xu·∫•t file PDF
    print("\n--- B·∫Øt ƒë·∫ßu xu·∫•t file PDF ---")
    actual_pdf_path = export_pdf(category_name, target_date, limit=None)

    # 5. T·∫£i l√™n Google Drive
    if actual_pdf_path and os.path.exists(actual_pdf_path):
        if DRIVE_FOLDER_ID:
            print("\n--- B·∫Øt ƒë·∫ßu t·∫£i l√™n Google Drive ---")
            upload_to_drive(actual_pdf_path, DRIVE_FOLDER_ID)
        else:
            print("\n‚ö†Ô∏è  B·ªè qua t·∫£i l√™n Drive: Bi·∫øn m√¥i tr∆∞·ªùng DRIVE_FOLDER_ID ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong file .env")
    else:
        print(f"\n‚ö†Ô∏è  B·ªè qua t·∫£i l√™n Drive: File PDF kh√¥ng ƒë∆∞·ª£c t·∫°o ho·∫∑c kh√¥ng t√¨m th·∫•y.")
    
    print("\nüéâüéâüéâ QUY TR√åNH HO√ÄN T·∫§T! üéâüéâüéâ")

def legacy_main():
    """
    H√†m ch√≠nh ƒëi·ªÅu ph·ªëi to√†n b·ªô qu√° tr√¨nh crawl cho nhi·ªÅu chuy√™n m·ª•c.
    """
    # L·∫•y chu·ªói JSON ch·ª©a c√°c m·ª•c ti√™u t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    targets_json = os.getenv("CRAWL_TARGETS")
    if not targets_json:
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng CRAWL_TARGETS. H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ c·∫•u h√¨nh n√≥ trong file .env ho·∫∑c GitHub Secrets.")
        return

    try:
        crawl_targets = json.loads(targets_json)
    except json.JSONDecodeError:
        print("‚ùå L·ªói: Bi·∫øn CRAWL_TARGETS kh√¥ng ph·∫£i l√† m·ªôt chu·ªói JSON h·ª£p l·ªá.")
        return

    # L·∫•y ng√†y hi·ªán t·∫°i theo m√∫i gi·ªù Vi·ªát Nam
    vietnam_tz = pytz.timezone("Asia/Ho_Chi_Minh")
    today = datetime.now(vietnam_tz)
    
    print(f"--- B·∫Øt ƒë·∫ßu phi√™n l√†m vi·ªác ng√†y {today.strftime('%d-%m-%Y')} ---")

    # L·∫∑p qua t·ª´ng m·ª•c ti√™u v√† th·ª±c hi·ªán crawl
    for target in crawl_targets:
        category_name = target.get("category_name")
        category_url = target.get("vnexpress_url")
        drive_folder_id = target.get("drive_folder_id")

        if not all([category_name, category_url, drive_folder_id]):
            print(f"‚ö†Ô∏è B·ªè qua m·ª•c ti√™u kh√¥ng h·ª£p l·ªá: {target}")
            continue
        
        try:
            crawl_articles_by_category_and_date(
                category_name=category_name,
                category_url=category_url,
                target_date=today,
                drive_folder_id=drive_folder_id,
                limit=10 # B·∫°n c√≥ th·ªÉ ƒë·∫∑t limit ·ªü ƒë√¢y
            )
        except Exception as e:
            print(f"‚ùå ƒê√£ x·∫£y ra l·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω chuy√™n m·ª•c '{category_name}': {e}")
        
        print(f"--- Ho√†n th√†nh chuy√™n m·ª•c: {category_name} ---\n")

    print("‚úÖ T·∫•t c·∫£ c√°c chuy√™n m·ª•c ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω. K·∫øt th√∫c phi√™n l√†m vi·ªác.")

if __name__ == "__main__":
    # B·∫°n ƒë√£ g·ªçi legacy_main() l√† ch√≠nh x√°c cho vi·ªác t·ª± ƒë·ªông h√≥a
    # main() 
    legacy_main()

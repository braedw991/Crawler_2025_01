from bs4 import BeautifulSoup
# N√ÇNG C·∫§P: S·ª≠ d·ª•ng Playwright ƒë·ªÉ ƒë·∫£m b·∫£o comment ƒë∆∞·ª£c t·∫£i
from utils.fetcher import fetch_soup_playwright
from utils.summarizer import summarize_with_gemini
from datetime import datetime
import pytz
import re
import traceback
import json
import requests

COMMENT_API_URL = "https://gw.vnexpress.net/index/get_comment"

def parse_created_at(raw: str):
    """
    X·ª≠ l√Ω ƒë·ªãnh d·∫°ng ng√†y VnExpress:
    - 'Th·ª© s√°u, 14/6/2025, 13:06 (GMT+7)'
    - 'Th·ª© s√°u, 14/6/2025 (GMT+7)'
    - '14/6/2025'
    """
    try:
        tz = pytz.timezone("Asia/Ho_Chi_Minh")
        parts = raw.strip().split(",")
        if len(parts) >= 3:
            date_part = parts[1].strip()
            time_part = parts[2].strip().split(" ")[0]
            dt = datetime.strptime(f"{date_part}, {time_part}", "%d/%m/%Y, %H:%M")
        elif len(parts) >= 2:
            date_part = parts[1].strip()
            dt = datetime.strptime(date_part, "%d/%m/%Y")
        else:
            dt = datetime.strptime(parts[0].strip(), "%d/%m/%Y")
        return tz.localize(dt)
    except Exception as e:
        print(f"[L·ªói ƒë·ªãnh d·∫°ng th·ªùi gian] {e}")
        return datetime.now(pytz.timezone("Asia/Ho_Chi_Minh"))

# N√ÇNG C·∫§P: ƒê∆°n gi·∫£n h√≥a h√†m, ch·ªâ parse t·ª´ soup ƒë√£ c√≥ s·∫µn
def fetch_and_parse_comments(soup: BeautifulSoup, limit: int = 3) -> list:
    """
    B√≥c t√°ch c√°c b√¨nh lu·∫≠n ƒë√£ ƒë∆∞·ª£c t·∫£i s·∫µn trong soup.
    H√†m n√†y gi·∫£ ƒë·ªãnh soup ƒë∆∞·ª£c cung c·∫•p b·ªüi Playwright v√† ƒë√£ ch·ª©a HTML c·ªßa c√°c b√¨nh lu·∫≠n.
    """
    comments_data = []
    try:
        # Selector tr·ª±c ti·∫øp ƒë·∫øn c√°c item b√¨nh lu·∫≠n
        comment_items = soup.select("#list_comment .comment_item")
        
        if not comment_items:
            print("‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y item b√¨nh lu·∫≠n n√†o trong HTML ƒë·ªÉ x·ª≠ l√Ω.")
            return []

        for item in comment_items:
            user_tag = item.select_one(".content-comment .nickname")
            content_tag = item.select_one(".content-comment p.full_content")
            like_tag = item.select_one(".reactions-total .number")

            if user_tag and content_tag:
                # X√≥a th·∫ª t√™n ng∆∞·ªùi d√πng kh·ªèi n·ªôi dung ƒë·ªÉ tr√°nh l·∫∑p l·∫°i
                if content_tag.find("span", class_="txt-name"):
                    content_tag.find("span", class_="txt-name").decompose()
                
                user = user_tag.get_text(strip=True)
                content = content_tag.get_text(strip=True, separator="\n").replace('"',"'")
                likes = int(like_tag.get_text(strip=True)) if like_tag and like_tag.get_text(strip=True).isdigit() else 0
                
                comments_data.append({"user": user, "content": content, "likes": likes})
        
        # S·∫Øp x·∫øp theo l∆∞·ª£t th√≠ch ƒë·ªÉ l·∫•y top comment
        comments_data.sort(key=lambda x: x['likes'], reverse=True)
        print(f"‚úÖ T√¨m th·∫•y v√† x·ª≠ l√Ω {len(comments_data)} b√¨nh lu·∫≠n.")
        return comments_data[:limit]

    except Exception as e:
        print(f"‚ö†Ô∏è  L·ªói khi b√≥c t√°ch b√¨nh lu·∫≠n: {e}")
        traceback.print_exc()
        return []

def is_valid_image_url(url: str) -> bool:
    """Lo·∫°i b·ªè ·∫£nh base64 ho·∫∑c placeholder kh√¥ng d√πng ƒë∆∞·ª£c"""
    return bool(url and url.startswith("http") and not url.startswith("data:image/"))

def extract_image_url(img_tag) -> str:
    """Tr√≠ch xu·∫•t URL h·ª£p l·ªá t·ª´ th·∫ª ·∫£nh, ∆∞u ti√™n c√°c thu·ªôc t√≠nh lazy-load."""
    if not img_tag:
        return None
        
    # ∆Øu ti√™n c√°c thu·ªôc t√≠nh lazy-loading
    for attr in ["data-src", "data-original", "data-srcset", "src"]:
        candidate = img_tag.get(attr)
        # data-srcset c√≥ th·ªÉ ch·ª©a nhi·ªÅu URL, l·∫•y c√°i ƒë·∫ßu ti√™n
        if candidate:
            first_url = candidate.strip().split(',')[0].split(' ')[0]
            if is_valid_image_url(first_url):
                return first_url
    return None

# N√ÇNG C·∫§P: S·ª≠ d·ª•ng Playwright ƒë·ªÉ l·∫•y soup
def parse_article(url: str, referer_url: str = None) -> dict:
    """
    Parse m·ªôt trang b√†i vi·∫øt c·ªßa VnExpress ƒë·ªÉ l·∫•y th√¥ng tin chi ti·∫øt.
    S·ª≠ d·ª•ng Playwright ƒë·ªÉ ƒë·∫£m b·∫£o n·ªôi dung ƒë·ªông (nh∆∞ b√¨nh lu·∫≠n) ƒë∆∞·ª£c t·∫£i.
    """
    try:
        # N√ÇNG C·∫§P: S·ª≠ d·ª•ng Playwright ƒë·ªÉ fetch trang, ch·ªù comment xu·∫•t hi·ªán
        print("üöÄ ƒêang t·∫£i b√†i vi·∫øt b·∫±ng Playwright ƒë·ªÉ l·∫•y b√¨nh lu·∫≠n...")
        # Ch·ªù t·ªëi ƒëa 15 gi√¢y cho selector c·ªßa comment xu·∫•t hi·ªán
        soup = fetch_soup_playwright(url, wait_for_selector="#list_comment .comment_item", timeout=15000)
        
        if not soup:
            # N·∫øu kh√¥ng c√≥ comment, th·ª≠ t·∫£i l·∫°i kh√¥ng c·∫ßn ch·ªù
            print("‚ö†Ô∏è Kh√¥ng th·∫•y b√¨nh lu·∫≠n, th·ª≠ t·∫£i l·∫°i trang c∆° b·∫£n...")
            soup = fetch_soup_playwright(url)
            if not soup:
                return None

        # --- Tr√≠ch xu·∫•t th√¥ng tin (nh∆∞ c≈©) ---
        title_tag = soup.find("h1", class_="title-detail")
        title = title_tag.get_text(strip=True) if title_tag else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"

        description_tag = soup.find("p", class_="description")
        description = description_tag.get_text(strip=True) if description_tag else ""

        content_tags = soup.select("article.fck_detail p.Normal")
        content = "\n".join(p.get_text(strip=True) for p in content_tags)

        author_tag = soup.find("p", style="text-align:right;")
        author = author_tag.get_text(strip=True) if author_tag else "Kh√¥ng r√µ"

        time_tag = soup.find("span", class_="date")
        date_str = time_tag.get_text(strip=True) if time_tag else ""
        created_at_dt = parse_created_at(date_str)
        
        image_url = None
        image_caption = ""
        main_image_tag = soup.select_one("article.fck_detail .fig-picture img, article.fck_detail picture img")
        if main_image_tag:
            image_url = extract_image_url(main_image_tag)
            fig_picture_div = main_image_tag.find_parent("div", class_="fig-picture")
            if fig_picture_div and fig_picture_div.get("data-sub-html"):
                sub_html_soup = BeautifulSoup(fig_picture_div["data-sub-html"], "html.parser")
                caption_tag = sub_html_soup.find("p", class_="Image")
                if caption_tag:
                    image_caption = caption_tag.get_text(separator=" ", strip=True)

        # L·∫•y c√°c b√¨nh lu·∫≠n n·ªïi b·∫≠t t·ª´ soup ƒë√£ ƒë∆∞·ª£c Playwright x·ª≠ l√Ω
        comments = fetch_and_parse_comments(soup, limit=3)

        summary = summarize_with_gemini(content) if content else ""

        return {
            "url": url,
            "title": title,
            "description": description,
            "content": content,
            "author": author,
            "created_at": created_at_dt.isoformat(),
            "image_url": image_url,
            "image_caption": image_caption,
            "summary": summary,
            "comments": comments, # Th√™m danh s√°ch b√¨nh lu·∫≠n
            "crawled_at": datetime.now(pytz.timezone("Asia/Ho_Chi_Minh")).isoformat()
        }

    except Exception as e:
        print(f"‚ùå L·ªói nghi√™m tr·ªçng khi parse b√†i vi·∫øt {url}: {e}")
        traceback.print_exc()
        return None

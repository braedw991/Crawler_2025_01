import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from utils.fetcher import fetch_soup # <-- Sá»­ dá»¥ng fetcher táº­p trung
from parsers.vnexpress_parser import parse_article
from database.db_manager import add_article
from datetime import datetime, time
import pytz
from urllib.parse import urljoin
import time as time_module
import random
from exporter.pdf_exporter import export_pdf
from integrations.google_drive_uploader import upload_to_drive
import os

# Import cÃ¡c thÃ nh pháº§n cáº§n thiáº¿t tá»« dá»± Ã¡n
from config.settings import BASE_URL, CATEGORIES
from utils.fetcher import fetch_soup_playwright # DÃ¹ng Playwright cho trang danh má»¥c

TIN_NONG_URL = "https://vnexpress.net/tin-nong"

def build_category_url(category_name: str, target_date: datetime) -> str:
    """
    XÃ¢y dá»±ng URL Ä‘á»™ng cho chuyÃªn má»¥c vÃ  ngÃ y cá»¥ thá»ƒ.
    VÃ­ dá»¥: https://vnexpress.net/category/day/cateid/1006219/...
    """
    cate_id = CATEGORIES.get(category_name)
    if not cate_id:
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y chuyÃªn má»¥c '{category_name}' trong settings.py")
        return None

    # Chuyá»ƒn Ä‘á»•i ngÃ y sang mÃºi giá» Viá»‡t Nam vÃ  láº¥y timestamp
    tz = pytz.timezone("Asia/Ho_Chi_Minh")
    start_of_day = tz.localize(datetime.combine(target_date.date(), time.min))
    end_of_day = tz.localize(datetime.combine(target_date.date(), time.max))
    
    from_timestamp = int(start_of_day.timestamp())
    to_timestamp = int(end_of_day.timestamp())

    # Cáº¥u trÃºc URL theo Ä‘á»‹nh dáº¡ng cá»§a VnExpress
    url_path = f"/category/day/cateid/{cate_id}/fromdate/{from_timestamp}/todate/{to_timestamp}/allcate/{cate_id}"
    return urljoin(BASE_URL, url_path)

def get_article_links_from_category_page(category_url: str) -> list:
    """
    Sá»­ dá»¥ng Playwright Ä‘á»ƒ táº£i trang danh má»¥c vÃ  láº¥y táº¥t cáº£ link bÃ i viáº¿t.
    """
    print(f"ğŸš€ Äang táº£i danh sÃ¡ch bÃ i viáº¿t tá»«: {category_url}")
    # Trang danh má»¥c cÃ³ thá»ƒ dÃ¹ng JavaScript Ä‘á»ƒ táº£i, Playwright lÃ  lá»±a chá»n an toÃ n
    soup = fetch_soup_playwright(category_url)
    if not soup:
        print("âŒ KhÃ´ng thá»ƒ táº£i trang danh sÃ¡ch bÃ i viáº¿t. Dá»«ng crawl.")
        return []
        
    links = []
    # --- Sá»¬A Lá»–I: Chá»‰ láº¥y link tá»« khu vá»±c ná»™i dung chÃ­nh ---
    # TÃ¬m vÃ¹ng chá»©a danh sÃ¡ch bÃ i viáº¿t chÃ­nh Ä‘á»ƒ trÃ¡nh láº¥y link tá»« sidebar "Xem nhiá»u".
    # Selector nÃ y tÃ¬m cÃ¡c bÃ i viáº¿t trong vÃ¹ng <div class="list-news-subfolder">,
    # lÃ  vÃ¹ng chá»©a ná»™i dung chÃ­nh cá»§a trang danh má»¥c.
    main_content_selector = "div.list-news-subfolder h3.title-news a[href], div.list-news-subfolder h2.title-news a[href]"
    
    for tag in soup.select(main_content_selector):
        href = tag.get("href")
        if href and (href.startswith("https://vnexpress.net") or href.startswith("/")):
            full_url = urljoin(BASE_URL, href)
            if ".html" in full_url and full_url not in links:
                links.append(full_url)

    print(f"âœ… TÃ¬m tháº¥y {len(links)} link bÃ i viáº¿t.")
    return links

# Sá»¬A Äá»”I 2: Thay Ä‘á»•i chá»¯ kÃ½ hÃ m Ä‘á»ƒ nháº­n 'category_url'
def crawl_articles_by_category_and_date(category_name: str, category_url: str, target_date: datetime, drive_folder_id: str, limit=10):
    """
    Quy trÃ¬nh crawl chÃ­nh: Láº¥y link tá»« URL Ä‘Æ°á»£c cung cáº¥p -> Parse tá»«ng bÃ i.
    """
    print(f"ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh crawl chuyÃªn má»¥c '{category_name}'...")
    
    # Sá»¬A Äá»”I 3: KhÃ´ng cáº§n build URL ná»¯a vÃ¬ nÃ³ Ä‘Ã£ Ä‘Æ°á»£c truyá»n vÃ o
    # category_url = build_category_url(category_name, target_date)
    if not category_url:
        print("âŒ URL chuyÃªn má»¥c khÃ´ng há»£p lá»‡. Dá»«ng láº¡i.")
        return

    # Sá»­ dá»¥ng category_url Ä‘Æ°á»£c truyá»n vÃ o Ä‘á»ƒ láº¥y link
    urls = get_article_links_from_category_page(category_url)
    if not urls:
        print("ğŸ KhÃ´ng cÃ³ bÃ i viáº¿t nÃ o Ä‘á»ƒ xá»­ lÃ½. Káº¿t thÃºc.")
        return

    print(f"ğŸ”— TÃ¬m tháº¥y tá»•ng cá»™ng {len(urls)} bÃ i viáº¿t Ä‘á»ƒ xá»­ lÃ½.")

    new_count = 0
    for idx, url in enumerate(urls, start=1):
        print(f"\nğŸ‘‰ Äang xá»­ lÃ½ bÃ i viáº¿t ({idx}/{len(urls)}): {url}")
        
        # Sá»¬A Lá»–I: Truyá»n category_url lÃ m referer cho hÃ m parse
        article = parse_article(url, referer_url=category_url)
        
        if not article:
            print("âŒ Lá»—i parse, bá» qua bÃ i viáº¿t nÃ y.")
            time_module.sleep(1)
            continue

        if add_article(article):
            print(f"âœ… ÄÃ£ lÆ°u bÃ i viáº¿t má»›i: {article['title']}")
            new_count += 1
        else:
            print(f"â© BÃ i viáº¿t Ä‘Ã£ tá»“n táº¡i, bá» qua.")

        # Táº¡m nghá»‰ ngáº«u nhiÃªn Ä‘á»ƒ trÃ¡nh bá»‹ block
        sleep_time = random.uniform(1.5, 3.5)
        print(f"--- Táº¡m nghá»‰ {sleep_time:.2f} giÃ¢y ---")
        time_module.sleep(sleep_time)

    print(f"\nğŸ‰ HoÃ n táº¥t! Tá»•ng sá»‘ bÃ i viáº¿t má»›i Ä‘Æ°á»£c lÆ°u: {new_count}")

    # 2. Xuáº¥t PDF
    pdf_file_path = export_pdf(category_name, target_date, limit=limit)
    
    # 3. Upload lÃªn Google Drive
    if pdf_file_path and os.path.exists(pdf_file_path):
        print(f"â˜ï¸  Äang táº£i file '{pdf_file_path}' lÃªn thÆ° má»¥c Google Drive ID: {drive_folder_id}")
        upload_to_drive(pdf_file_path, drive_folder_id)
    else:
        print("âš ï¸ KhÃ´ng cÃ³ file PDF Ä‘á»ƒ táº£i lÃªn.")
